#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨ - æ¢å¤åŸå§‹å¯åŠ¨é€Ÿåº¦
ä½¿ç”¨ Whisper-large-v3 + Qwen2.5-7B-Instruct ç»„åˆ

Copyright (c) 2025 superjackche
GitHub: https://github.com/superjackche/audio_ai
Licensed under the MIT License. See LICENSE file for details.
"""

import os
import sys
import torch
import time
import hashlib
import logging
import librosa
import numpy as np
import json
import re
import glob
from typing import Optional, List, Dict, Any
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
_current_file_dir = os.path.dirname(__file__)
_project_root = os.path.abspath(os.path.join(_current_file_dir, '..'))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    WhisperProcessor, 
    WhisperForConditionalGeneration,
    pipeline
)

# FunASR ç›¸å…³å¯¼å…¥
try:
    from funasr import AutoModel
    FUNASR_AVAILABLE = True
    print(f"âœ… FunASRåº“å¯¼å…¥æˆåŠŸï¼ŒAutoModelå¯ç”¨")
except ImportError as e:
    FUNASR_AVAILABLE = False
    print(f"âŒ FunASRåº“å¯¼å…¥å¤±è´¥: {e}")
    print("   ğŸ’¡ è¯·ç¡®è®¤FunASRå·²æ­£ç¡®å®‰è£…: pip install funasr")

from config.settings import MODEL_CONFIG, DATA_PATHS

class SimpleModelManager:
    """ç®€åŒ–çš„å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"DATA_PATHS['models_dir'] is configured as: {DATA_PATHS['models_dir']}")
        
        # æ˜¾ç¤ºFunASRçŠ¶æ€
        print(f"ğŸ”§ FunASRçŠ¶æ€æ£€æŸ¥: {'âœ… å¯ç”¨' if FUNASR_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        
        # Whisperæ¨¡å‹ç”¨äºè¯­éŸ³è½¬æ–‡å­—
        self.whisper_pipeline: Optional[Any] = None
        
        # Qwen2.5æ¨¡å‹ç”¨äºæ–‡æœ¬åˆ†æ
        self.llm_model: Optional[Any] = None
        self.llm_tokenizer: Optional[AutoTokenizer] = None
        
        self.device = self._get_device()
        
        # ç®€å•ç¼“å­˜
        self.inference_cache = {}
        
        # æ¨¡å‹é…ç½® - åªä½¿ç”¨SenseVoiceå’ŒLLM
        # ä½¿ç”¨SenseVoice - ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–çš„è½»é‡çº§ASRæ¨¡å‹ï¼ˆä»…50MBï¼‰
        self.whisper_model_name = "iic/SenseVoiceSmall"
        self.use_sensevoice = True  # åªä½¿ç”¨SenseVoiceï¼Œä¸ä½¿ç”¨Whisper
        self.skip_whisper = True    # è·³è¿‡Whisperä¸‹è½½
        
        # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„æœ¬åœ°LLMæ¨¡å‹è·¯å¾„ï¼ŒåŒ…æ‹¬HuggingFaceç¼“å­˜
        cache_dir = DATA_PATHS['models_dir'] / 'cache'
        possible_paths = [
            DATA_PATHS['models_dir'] / "Qwen2.5-7B-Instruct",
            DATA_PATHS['models_dir'] / "Qwen--Qwen2.5-7B-Instruct", 
            Path("/new_disk/cwh/models/Qwen2.5-7B-Instruct"),
            Path("/new_disk/cwh/models/Qwen--Qwen2.5-7B-Instruct"),
            # æ£€æŸ¥HuggingFaceç¼“å­˜ç›®å½•
            cache_dir / "models--Qwen--Qwen2.5-7B-Instruct" / "snapshots",
        ]
        
        self.llm_model_path = None
        self.llm_model_name = "Qwen/Qwen2.5-7B-Instruct"  # ç”¨äºåœ¨çº¿ä¸‹è½½çš„fallback
        
        # å¯»æ‰¾æœ¬åœ°æ¨¡å‹
        print("ğŸ” æ£€æŸ¥æœ¬åœ°LLMæ¨¡å‹...")
        for path in possible_paths:
            print(f"  æ£€æŸ¥è·¯å¾„: {path}")
            if path.exists() and path.is_dir():
                # å¯¹äºHuggingFaceç¼“å­˜ç›®å½•ï¼Œéœ€è¦è¿›å…¥snapshotså­ç›®å½•
                if "snapshots" in str(path):
                    # æŸ¥æ‰¾snapshotsç›®å½•ä¸‹çš„å…·ä½“ç‰ˆæœ¬
                    snapshot_dirs = [d for d in path.iterdir() if d.is_dir()]
                    if snapshot_dirs:
                        # ä½¿ç”¨æœ€æ–°çš„snapshot
                        latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                        path = latest_snapshot
                        print(f"    å‘ç°ç¼“å­˜å¿«ç…§: {latest_snapshot.name}")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
                required_files = ["config.json"]
                
                # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶ï¼šæ”¯æŒå¤šç§æ ¼å¼
                has_single_model = any((path / f).exists() for f in ["model.safetensors", "pytorch_model.bin"])
                has_sharded_safetensors = any((path / f"model-{i:05d}-of-*.safetensors").exists() for i in range(1, 10))
                has_sharded_pytorch = any((path / f"pytorch_model-{i:05d}-of-*.bin").exists() for i in range(1, 10))
                
                # æ–°å¢ï¼šæ£€æŸ¥åˆ†ç‰‡å‘½åæ ¼å¼å¦‚ model-00001-of-00004.safetensors
                sharded_files = glob.glob(str(path / "model-*-of-*.safetensors"))
                has_new_sharded_format = len(sharded_files) > 0
                
                has_model_files = has_single_model or has_sharded_safetensors or has_sharded_pytorch or has_new_sharded_format
                has_tokenizer = any((path / f).exists() for f in ["tokenizer.json", "tokenizer_config.json"])
                
                # è°ƒè¯•ä¿¡æ¯
                print(f"    æ¨¡å‹æ–‡ä»¶æ£€æŸ¥: å•æ–‡ä»¶={has_single_model}, æ—§åˆ†ç‰‡={has_sharded_safetensors or has_sharded_pytorch}, æ–°åˆ†ç‰‡={has_new_sharded_format}")
                print(f"    tokenizeræ£€æŸ¥: {has_tokenizer}")
                if has_new_sharded_format:
                    print(f"    å‘ç°åˆ†ç‰‡æ–‡ä»¶: {len(sharded_files)}ä¸ª")
                
                if all((path / f).exists() for f in required_files) and has_model_files and has_tokenizer:
                    self.llm_model_path = path
                    print(f"  âœ… æ‰¾åˆ°å®Œæ•´çš„æœ¬åœ°LLMæ¨¡å‹: {path}")
                    break
                else:
                    print(f"  âš ï¸  è·¯å¾„å­˜åœ¨ä½†æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {path}")
                    print(f"    ç¼ºå°‘: config.json={not all((path / f).exists() for f in required_files)}, model_files={not has_model_files}, tokenizer={not has_tokenizer}")
                    # è°ƒè¯•ï¼šåˆ—å‡ºç›®å½•å†…å®¹
                    try:
                        files = list(path.iterdir())[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                        print(f"    ç›®å½•å†…å®¹ç¤ºä¾‹: {[f.name for f in files]}")
                    except:
                        pass
            else:
                print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
        
        if not self.llm_model_path:
            print(f"  ğŸ“¥ æœªæ‰¾åˆ°æœ¬åœ°LLMæ¨¡å‹ï¼Œå°†ä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹: {self.llm_model_name}")
            print(f"  ğŸ’¡ ç¼“å­˜ç›®å½•: {cache_dir}")
            # å³ä½¿æ²¡æœ‰æ‰¾åˆ°å®Œæ•´è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ¨¡å‹åç§°ï¼Œè®©transformersè‡ªåŠ¨æ‰¾ç¼“å­˜
        
        self.logger.info(f"ç®€åŒ–æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _get_device(self) -> str:
        """è·å–è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            device = f"cuda:{torch.cuda.current_device()}"
            self.logger.info(f"ä½¿ç”¨GPU: {device}")
            return device
        else:
            self.logger.info("ä½¿ç”¨CPU")
            return "cpu"
    
    def load_whisper_model(self) -> bool:
        """å¿«é€ŸåŠ è½½SenseVoiceè¯­éŸ³è¯†åˆ«æ¨¡å‹ - ä¸ä½¿ç”¨Whisper"""
        try:
            # å¼ºåˆ¶æ£€æŸ¥FunASRçŠ¶æ€
            print(f"ğŸ” æ£€æŸ¥FunASRçŠ¶æ€: FUNASR_AVAILABLE = {FUNASR_AVAILABLE}")
            
            if not FUNASR_AVAILABLE:
                print("âŒ FunASRåº“ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨SenseVoice")
                print("   ğŸ’¡ è¯·å®‰è£…FunASR: pip install funasr")
                return False
            
            print(f"ğŸ™ï¸  åŠ è½½SenseVoiceä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹: {self.whisper_model_name}")
            print("   âš¡ SenseVoiceæ¨¡å‹ä»…50MBï¼Œä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–ï¼ŒåŠ è½½è¶…å¿«ï¼")
            
            self.logger.info(f"æ­£åœ¨ä½¿ç”¨FunASRåŠ è½½SenseVoiceæ¨¡å‹: {self.whisper_model_name}")
            
            # ä½¿ç”¨FunASRçš„AutoModelåŠ è½½SenseVoice
            # æ ¹æ®æ–‡æ¡£ï¼Œéœ€è¦è®¾ç½®æ­£ç¡®çš„å‚æ•°
            model_kwargs = {
                "trust_remote_code": True,
                "device": self.device,
                "disable_update": True  # ç¦ç”¨æ›´æ–°æ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨
            }
            
            # æ·»åŠ é•œåƒæºç¯å¢ƒå˜é‡æ”¯æŒ
            if 'HF_ENDPOINT' in os.environ:
                model_kwargs["hub_base_url"] = os.environ['HF_ENDPOINT']
            
            self.whisper_pipeline = AutoModel(
                model=self.whisper_model_name,
                **model_kwargs
            )
            
            print("âœ… SenseVoiceä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")
            self.logger.info("SenseVoiceæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ SenseVoiceæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.logger.error(f"SenseVoiceæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_llm_model(self) -> bool:
        """å¿«é€ŸåŠ è½½LLMæ¨¡å‹"""
        try:
            if self.llm_model_path:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                print(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹: {self.llm_model_path}")
                self.logger.info(f"ä»æœ¬åœ°è·¯å¾„åŠ è½½LLMæ¨¡å‹: {self.llm_model_path}")
                model_path_str = str(self.llm_model_path)
                use_local_only = True
            else:
                # ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                print(f"ğŸ“¥ éœ€è¦ä¸‹è½½LLMæ¨¡å‹: {self.llm_model_name}")
                print("   ğŸ“¦ æ¨¡å‹å¤§å°çº¦13GBï¼Œè¿™å°†éœ€è¦è¾ƒé•¿æ—¶é—´...")
                print("   ğŸ’¡ å»ºè®®ï¼šå°†æœ¬åœ°7Bæ¨¡å‹æ”¾ç½®åœ¨ä»¥ä¸‹ä»»ä¸€è·¯å¾„ï¼š")
                print("      - /new_disk/cwh/models/Qwen2.5-7B-Instruct/")
                print("      - /new_disk/cwh/audio_ai/models/Qwen2.5-7B-Instruct/")
                print("   â³ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
                self.logger.info(f"ä»ç½‘ç»œä¸‹è½½LLMæ¨¡å‹: {self.llm_model_name}")
                model_path_str = self.llm_model_name
                use_local_only = False

            # åŠ è½½tokenizer
            print("ğŸ”§ åŠ è½½Tokenizer...")
            tokenizer_kwargs = {
                "trust_remote_code": True,
                "cache_dir": str(DATA_PATHS['models_dir'] / 'cache'),
            }
            if use_local_only:
                tokenizer_kwargs["local_files_only"] = True
            else:
                # è®¾ç½®é•œåƒæºå’Œä¸‹è½½é…ç½®
                tokenizer_kwargs["local_files_only"] = False
                tokenizer_kwargs["resume_download"] = True
                print("   ğŸŒ ä½¿ç”¨hf-mirroré•œåƒæºä¸‹è½½...")
            
            try:
                self.llm_tokenizer = AutoTokenizer.from_pretrained(
                    model_path_str,
                    **tokenizer_kwargs
                )
                print("âœ… TokenizeråŠ è½½æˆåŠŸ")
            except Exception as e:
                if not use_local_only:
                    print(f"   âš ï¸  Tokenizerä¸‹è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                    print(f"   ğŸ”„ é‡è¯•ä¸­... (é”™è¯¯: {e})")
                    # é‡è¯•ä¸€æ¬¡ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
                    import time
                    time.sleep(2)
                    self.llm_tokenizer = AutoTokenizer.from_pretrained(
                        model_path_str,
                        **tokenizer_kwargs
                    )
                    print("âœ… Tokenizerä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
                else:
                    raise e
            
            # åŠ è½½æ¨¡å‹
            print("ğŸš€ åŠ è½½LLMæ¨¡å‹æƒé‡...")
            if not use_local_only:
                print("   â³ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                print("   ğŸ“Š ä¸‹è½½è¿›åº¦å°†åœ¨å‘½ä»¤è¡Œæ˜¾ç¤º...")
            
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if self.device.startswith("cuda") else torch.float32,
                "low_cpu_mem_usage": True,
                "cache_dir": str(DATA_PATHS['models_dir'] / 'cache'),
            }
            
            if use_local_only:
                model_kwargs["local_files_only"] = True
            else:
                model_kwargs["local_files_only"] = False
                model_kwargs["resume_download"] = True
                # å¼ºåˆ¶ä½¿ç”¨é•œåƒæº
                model_kwargs["proxies"] = None
            
            if self.device.startswith("cuda"):
                model_kwargs["device_map"] = "auto"
            
            try:
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_path_str,
                    **model_kwargs
                )
            except Exception as e:
                if not use_local_only and "offline" not in str(e).lower():
                    print(f"   âš ï¸  æ¨¡å‹ä¸‹è½½å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                    print(f"   ğŸ”„ é‡è¯•ä¸‹è½½... (é”™è¯¯: {e})")
                    # é‡è¯•ä¸€æ¬¡ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜
                    import time
                    time.sleep(5)
                    self.llm_model = AutoModelForCausalLM.from_pretrained(
                        model_path_str,
                        **model_kwargs
                    )
                else:
                    raise e
            
            self.llm_model.eval()
            
            print("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
            if not use_local_only:
                print(f"   ğŸ’¾ æ¨¡å‹å·²ç¼“å­˜åˆ°: {DATA_PATHS['models_dir']}/cache")
            self.logger.info(f"LLMæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path_str}")
            return True
            
        except Exception as e: 
            print(f"âŒ LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
            print("      1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("      2. æ£€æŸ¥ç£ç›˜ç©ºé—´ (éœ€è¦çº¦20GB)")
            print("      3. ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
            print("      4. å°è¯•ä½¿ç”¨VPNæˆ–æ›´æ¢ç½‘ç»œ")
            self.logger.error(f"LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            return False
    
    def initialize_models(self) -> bool:
        """å¿«é€Ÿåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ - åªåŠ è½½SenseVoiceå’ŒLLM"""
        print("\nğŸ¤– å¼€å§‹åˆå§‹åŒ–AIæ¨¡å‹...")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        self.logger.info("å¼€å§‹åˆå§‹åŒ–æ¨¡å‹...")
        
        # åŠ è½½SenseVoice
        print("ğŸ™ï¸  æ­£åœ¨åŠ è½½SenseVoiceè¯­éŸ³è¯†åˆ«æ¨¡å‹...")
        if not self.load_whisper_model():
            print("âŒ SenseVoiceæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åŠ è½½LLM
        print("\nğŸ§  æ­£åœ¨åŠ è½½LLMæ–‡æœ¬åˆ†ææ¨¡å‹...")
        if not self.load_llm_model():
            print("âŒ LLMæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
        
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ‰ æ‰€æœ‰AIæ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
        print("   âœ… SenseVoiceä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹å·²å°±ç»ª")
        print("   âœ… Qwen2.5-7Bæ–‡æœ¬åˆ†ææ¨¡å‹å·²å°±ç»ª")
        self.logger.info("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:
        """è¯­éŸ³è½¬æ–‡å­—ï¼Œä½¿ç”¨SenseVoiceè¿›è¡Œä¸­è‹±æ–‡æ··åˆè¯†åˆ«"""
        # æ‡’åŠ è½½æ£€æŸ¥
        if not self.whisper_pipeline:
            print("ğŸ™ï¸  é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨åŠ è½½SenseVoiceæ¨¡å‹...")
            if not self.load_whisper_model():
                raise RuntimeError("è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥")
        
        try:
            start_time = time.time()
            
            # ä½¿ç”¨FunASRçš„SenseVoiceæ¨¡å‹è¿›è¡Œè¯†åˆ«
            self.logger.info("ä½¿ç”¨SenseVoiceæ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
            
            # SenseVoiceä½¿ç”¨FunASRæ¥å£
            result = self.whisper_pipeline.generate(
                input=audio_path,
                cache={},
                language="auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼Œæ”¯æŒä¸­è‹±æ··åˆ
                use_itn=True,     # ä½¿ç”¨é€†æ–‡æœ¬æ ‡å‡†åŒ–
                batch_size_s=60  # æ‰¹å¤„ç†å¤§å°
            )
            
            # å¤„ç†FunASRçš„è¿”å›ç»“æœ
            if isinstance(result, list) and len(result) > 0:
                transcription = result[0].get("text", "").strip()
            else:
                transcription = ""
                    
            transcribe_time = time.time() - start_time
            
            # æ£€æµ‹è¯­è¨€
            detected_language = self._detect_language(transcription)
            
            self.logger.info(f"è¯­éŸ³è½¬æ–‡å­—å®Œæˆï¼Œè€—æ—¶: {transcribe_time:.2f}ç§’")
            self.logger.info(f"æ£€æµ‹è¯­è¨€: {detected_language}, è½¬å½•é•¿åº¦: {len(transcription)}å­—ç¬¦")
            
            return {
                "text": transcription,
                "language": detected_language,
                "processing_time": transcribe_time,
                "method": "sensevoice"
            }
            
        except Exception as e:
            self.logger.error(f"è¯­éŸ³è½¬æ–‡å­—å¤±è´¥: {e}")
            raise
    
    def analyze_text_risk(self, text: str) -> Dict[str, Any]:
        """æ–‡æœ¬é£é™©åˆ†æ"""
        # æ‡’åŠ è½½æ£€æŸ¥
        if not self.llm_model or not self.llm_tokenizer:
            print("ğŸ§  é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨åŠ è½½LLMæ¨¡å‹...")
            if not self.load_llm_model():
                raise RuntimeError("LLMæ¨¡å‹åŠ è½½å¤±è´¥")
        
        try:
            start_time = time.time()
            
            # æ£€æŸ¥ç®€å•ç¼“å­˜
            cache_key = hashlib.md5(text.encode()).hexdigest()
            if cache_key in self.inference_cache:
                return self.inference_cache[cache_key]
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_risk_analysis_prompt(text)
            
            # ä½¿ç”¨chatæ¨¡æ¿æ ¼å¼
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹é£é™©åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            # åº”ç”¨chatæ¨¡æ¿
            formatted_prompt = self.llm_tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = self.llm_tokenizer(
                formatted_prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=2048
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device.startswith("cuda"):
                inputs = inputs.to(self.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=self.llm_tokenizer.eos_token_id
                )
            
            # è§£ç 
            input_length = inputs["input_ids"].shape[1]
            generated_ids = outputs[:, input_length:]
            response = self.llm_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            analysis_time = time.time() - start_time
            
            # è§£æç»“æœ
            risk_analysis = self._parse_risk_analysis(response)
            risk_analysis["processing_time"] = analysis_time
            
            # ç®€å•ç¼“å­˜ï¼ˆé™åˆ¶å¤§å°ï¼‰
            if len(self.inference_cache) < 100:
                self.inference_cache[cache_key] = risk_analysis
            
            self.logger.info(f"æ–‡æœ¬åˆ†æå®Œæˆï¼Œè€—æ—¶: {analysis_time:.2f}ç§’")
            
            return risk_analysis
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬åˆ†æå¤±è´¥: {e}")
            raise
    
    def process_audio_complete(self, audio_path: str) -> Dict[str, Any]:
        """å®Œæ•´çš„éŸ³é¢‘å¤„ç†æµç¨‹"""
        try:
            total_start_time = time.time()
            
            # 1. è¯­éŸ³è½¬æ–‡å­—
            transcription_result = self.transcribe_audio(audio_path)
            text = transcription_result["text"]
            
            if not text.strip():
                return {
                    "text": "",
                    "language": "zh",
                    "risk_analysis": {
                        "risk_level": "æ— å†…å®¹",
                        "risk_score": 0,
                        "key_issues": ["éŸ³é¢‘æ— æœ‰æ•ˆè¯­éŸ³å†…å®¹"],
                        "suggestions": ["è¯·æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶"],
                        "detailed_analysis": "éŸ³é¢‘æ–‡ä»¶ä¸­æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³å†…å®¹"
                    },
                    "processing_method": "simple_fast",
                    "total_processing_time": time.time() - total_start_time
                }
            
            # 2. æ–‡æœ¬é£é™©åˆ†æ
            risk_analysis = self.analyze_text_risk(text)
            
            total_time = time.time() - total_start_time
            
            return {
                "text": text,
                "language": transcription_result.get("language", "zh"),
                "risk_analysis": risk_analysis,
                "processing_method": "simple_fast",
                "transcription_time": transcription_result.get("processing_time", 0),
                "analysis_time": risk_analysis.get("processing_time", 0),
                "total_processing_time": total_time
            }
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _build_risk_analysis_prompt(self, text: str) -> str:
        """æ„å»ºé£é™©åˆ†ææç¤ºè¯"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å†…å®¹æ˜¯å¦å­˜åœ¨æ”¿æ²»é£é™©æˆ–æ„è¯†å½¢æ€é—®é¢˜ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{text}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«æ”¿æ²»æ•æ„Ÿå†…å®¹
2. è¯„ä¼°æ„è¯†å½¢æ€å€¾å‘  
3. æ£€æµ‹ä»·å€¼è§‚å†²çª
4. ç»™å‡ºé£é™©ç­‰çº§ï¼ˆä½é£é™©/ä¸­é£é™©/é«˜é£é™©ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "risk_level": "é£é™©ç­‰çº§",
    "risk_score": é£é™©åˆ†æ•°(0-100),
    "key_issues": ["å…³é”®é—®é¢˜åˆ—è¡¨"],
    "suggestions": ["æ”¹è¿›å»ºè®®"],
    "detailed_analysis": "è¯¦ç»†åˆ†æ"
}}

æ³¨æ„ï¼šåªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚"""
        
        return prompt
    
    def _parse_risk_analysis(self, response: str) -> Dict[str, Any]:
        """è§£æé£é™©åˆ†æç»“æœ"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed_result = json.loads(json_str)
                
                # ç¡®ä¿åŒ…å«å¿…è¦å­—æ®µ
                required_fields = ["risk_level", "risk_score", "key_issues", "suggestions", "detailed_analysis"]
                for field in required_fields:
                    if field not in parsed_result:
                        parsed_result[field] = "æœªçŸ¥" if field == "risk_level" else (0 if field == "risk_score" else [])
                
                return parsed_result
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
                return self._extract_info_from_text(response)
                
        except Exception as e:
            self.logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {e}")
            return {
                "risk_level": "è§£æé”™è¯¯",
                "risk_score": 0,
                "key_issues": ["ç»“æœè§£æå¤±è´¥"],
                "suggestions": ["è¯·é‡æ–°åˆ†æ"],
                "detailed_analysis": response,
                "error": str(e)
            }
    
    def _extract_info_from_text(self, response: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–é£é™©ä¿¡æ¯"""
        result = {
            "risk_level": "æœªçŸ¥",
            "risk_score": 0,
            "key_issues": [],
            "suggestions": [],
            "detailed_analysis": response
        }
        
        response_lower = response.lower()
        
        # æå–é£é™©ç­‰çº§
        if 'é«˜é£é™©' in response or 'high risk' in response_lower:
            result["risk_level"] = "é«˜é£é™©"
            result["risk_score"] = 80
        elif 'ä¸­é£é™©' in response or 'medium risk' in response_lower:
            result["risk_level"] = "ä¸­é£é™©"
            result["risk_score"] = 50
        elif 'ä½é£é™©' in response or 'low risk' in response_lower:
            result["risk_level"] = "ä½é£é™©"
            result["risk_score"] = 20
        
        return result
    
    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            if not text or not text.strip():
                return "unknown"
            
            # ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            english_chars = len([c for c in text if c.isalpha() and c.isascii()])
            total_chars = len(text.strip())
            
            if total_chars == 0:
                return "unknown"
            
            chinese_ratio = chinese_chars / total_chars
            english_ratio = english_chars / total_chars
            
            # åˆ¤æ–­ä¸»è¦è¯­è¨€
            if chinese_ratio > 0.3:
                if english_ratio > 0.2:
                    return "zh-en"  # ä¸­è‹±æ··åˆ
                else:
                    return "zh"     # ä¸»è¦æ˜¯ä¸­æ–‡
            elif english_ratio > 0.5:
                return "en"         # ä¸»è¦æ˜¯è‹±æ–‡
            else:
                return "mixed"      # å…¶ä»–æ··åˆæƒ…å†µ
                
        except Exception as e:
            self.logger.warning(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
            return "unknown"
