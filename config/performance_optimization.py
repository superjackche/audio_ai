#!/usr/bin/env python3
"""
ğŸš€ AIè¯­éŸ³æ”¿æ²»é£é™©ç›‘æ§ç³»ç»Ÿ - æé€Ÿæ€§èƒ½ä¼˜åŒ–é…ç½®
==========================================

ä¸“ä¸ºè¶…é«˜é€Ÿã€è¶…å‡†ç¡®çš„è¯­éŸ³å¤„ç†è€Œè®¾è®¡çš„æ€§èƒ½ä¼˜åŒ–é…ç½®
ç›®æ ‡ï¼šæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ï¼Œæœ€å°åŒ–å»¶è¿Ÿï¼Œæœ€ä¼˜åŒ–å†…å­˜ä½¿ç”¨

ä¼˜åŒ–é‡ç‚¹ï¼š
1. æ¨¡å‹åŠ è½½ä¼˜åŒ– - é¢„åŠ è½½ã€é‡åŒ–ã€ç¼“å­˜
2. GPUå†…å­˜ä¼˜åŒ– - åŠ¨æ€åˆ†é…ã€å†…å­˜æ± 
3. å¹¶å‘å¤„ç†ä¼˜åŒ– - å¼‚æ­¥å¤„ç†ã€æ‰¹å¤„ç†
4. éŸ³é¢‘å¤„ç†ä¼˜åŒ– - å®æ—¶æµå¼å¤„ç†
5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ– - æ™ºèƒ½ç¼“å­˜ã€é¢„æµ‹åŠ è½½
"""

import os
import time
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import psutil

# ==================== æ—©æœŸPyTorché…ç½® ====================
# å¿…é¡»åœ¨ä»»ä½•PyTorchæ“ä½œä¹‹å‰è®¾ç½®ï¼

def setup_torch_early_optimization():
    """åœ¨PyTorchåˆå§‹åŒ–å‰è®¾ç½®å…³é”®é…ç½®"""
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨å¯¼å…¥torchå‰ï¼‰
    cpu_cores = psutil.cpu_count(logical=False)
    os.environ['OMP_NUM_THREADS'] = str(cpu_cores)
    os.environ['MKL_NUM_THREADS'] = str(cpu_cores)
    os.environ['NUMEXPR_NUM_THREADS'] = str(cpu_cores)
    
    # è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
    
    return cpu_cores

# æ—©æœŸè®¾ç½®
_CPU_CORES = setup_torch_early_optimization()

# ç°åœ¨å®‰å…¨å¯¼å…¥PyTorch
import torch
import GPUtil

# ==================== ç³»ç»Ÿæ£€æµ‹ä¸è‡ªé€‚åº”é…ç½® ====================

def detect_system_capabilities():
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èƒ½åŠ›å¹¶è¿”å›ä¼˜åŒ–é…ç½®"""
    
    # GPUæ£€æµ‹
    gpu_info = {
        'available': torch.cuda.is_available(),
        'count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
        'memory': [],
        'compute_capability': []
    }
    
    if gpu_info['available']:
        for i in range(gpu_info['count']):
            props = torch.cuda.get_device_properties(i)
            gpu_info['memory'].append(props.total_memory // (1024**3))  # GB
            gpu_info['compute_capability'].append(f"{props.major}.{props.minor}")
    
    # CPUæ£€æµ‹
    cpu_info = {
        'cores': _CPU_CORES,  # ä½¿ç”¨æ—©æœŸæ£€æµ‹çš„CPUæ ¸å¿ƒæ•°
        'threads': psutil.cpu_count(logical=True),
        'frequency': psutil.cpu_freq().max if psutil.cpu_freq() else 0,
        'memory_gb': psutil.virtual_memory().total // (1024**3)
    }
    
    return gpu_info, cpu_info

def display_system_info():
    """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
    try:
        gpu_info, cpu_info = detect_system_capabilities()
        
        print("ğŸ“Š ç³»ç»Ÿé…ç½®ä¿¡æ¯:")
        print(f"   CPU: {cpu_info['cores']}ç‰©ç†æ ¸å¿ƒ / {cpu_info['threads']}é€»è¾‘çº¿ç¨‹")
        print(f"   å†…å­˜: {cpu_info['memory_gb']}GB")
        print(f"   CPUé¢‘ç‡: {cpu_info['frequency']:.1f}MHz" if cpu_info['frequency'] else "   CPUé¢‘ç‡: æœªçŸ¥")
        
        if gpu_info['available']:
            print(f"   GPU: {gpu_info['count']}ä¸ªå¯ç”¨")
            for i, mem in enumerate(gpu_info['memory']):
                compute_cap = gpu_info['compute_capability'][i] if i < len(gpu_info['compute_capability']) else "æœªçŸ¥"
                print(f"   GPU {i}: {mem}GBæ˜¾å­˜, è®¡ç®—èƒ½åŠ› {compute_cap}")
        else:
            print("   GPU: æ— å¯ç”¨GPU")
            
        # æ˜¾ç¤ºGPUå®æ—¶çŠ¶æ€
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                print("   GPUå®æ—¶çŠ¶æ€:")
                for gpu in gpus:
                    print(f"     GPU {gpu.id}: {gpu.name}, ä½¿ç”¨ç‡ {gpu.load*100:.1f}%, å†…å­˜ {gpu.memoryUsed}MB/{gpu.memoryTotal}MB")
        except Exception as e:
            print(f"   GPUçŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"ç³»ç»Ÿä¿¡æ¯æ˜¾ç¤ºå¤±è´¥: {e}")

# ==================== æé€Ÿæ¨¡å‹é…ç½® ====================

def get_ultra_fast_model_config():
    """è·å–è¶…é«˜é€Ÿæ¨¡å‹é…ç½®"""
    
    gpu_info, cpu_info = detect_system_capabilities()
    
    # åŸºç¡€é…ç½®
    config = {
        "model_optimization": {
            # æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆæ›´å¿«çš„3Bæ¨¡å‹
            "primary_model": "Qwen/Qwen2.5-Omni-3B",
            "fallback_model": "Qwen/Qwen2.5-Omni-7B",
            
            # é‡åŒ–é…ç½®
            "quantization": {
                "enabled": True,
                "method": "bitsandbytes",  # ä½¿ç”¨4bité‡åŒ–
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": "float16",  # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯torch.dtype
                "bnb_4bit_use_double_quant": True,
                "bnb_4bit_quant_type": "nf4"
            },
            
            # æ¨¡å‹åŠ è½½ä¼˜åŒ–
            "loading": {
                "torch_dtype": "float16",  # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯torch.dtype
                "device_map": "auto",
                "low_cpu_mem_usage": True,
                "trust_remote_code": True,
                "use_flash_attention_2": True,  # Flash Attention 2.0
                "attn_implementation": "flash_attention_2"
            }
        },
        
        "gpu_optimization": {
            # GPUå†…å­˜ç®¡ç†
            "memory_management": {
                "max_memory_fraction": 0.85,  # ä½¿ç”¨85%GPUå†…å­˜
                "memory_growth": True,
                "allow_growth": True,
                "pre_allocate": False
            },
            
            # CUDAä¼˜åŒ–
            "cuda_settings": {
                "cudnn_benchmark": True,
                "cudnn_deterministic": False,
                "empty_cache_frequency": 10,  # æ¯10æ¬¡æ¨ç†æ¸…ç†ç¼“å­˜
                "use_cuda_graphs": True
            }
        },
        
        "inference_optimization": {
            # æ¨ç†å‚æ•°ä¼˜åŒ–
            "generation_config": {
                "max_new_tokens": 512,
                "temperature": 0.1,  # æ›´ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
                "top_p": 0.9,
                "top_k": 50,
                "do_sample": True,
                "pad_token_id": 0,
                "eos_token_id": 1,
                "use_cache": True
            },
            
            # æ‰¹å¤„ç†é…ç½®
            "batch_processing": {
                "enabled": True,
                "batch_size": 4 if gpu_info['memory'] and gpu_info['memory'][0] > 20 else 2,
                "max_batch_size": 8,
                "dynamic_batching": True,
                "timeout_seconds": 0.1
            }
        }
    }
    
    # æ ¹æ®GPUæ•°é‡è°ƒæ•´é…ç½®
    if gpu_info['count'] >= 2:
        config["model_optimization"]["loading"]["device_map"] = {
            "": 0,  # ä¸»æ¨¡å‹æ”¾åœ¨GPU 0
            "lm_head": 1  # è¾“å‡ºå±‚æ”¾åœ¨GPU 1
        }
        config["inference_optimization"]["batch_processing"]["batch_size"] *= 2
    
    return config

# ==================== éŸ³é¢‘å¤„ç†ä¼˜åŒ–é…ç½® ====================

AUDIO_PROCESSING_CONFIG = {
    "streaming": {
        "enabled": True,
        "chunk_size": 1024,  # æ›´å°çš„å—å¤§å°å‡å°‘å»¶è¿Ÿ
        "overlap": 0.2,
        "buffer_size": 4096,
        "sample_rate": 16000,
        "channels": 1
    },
    
    "preprocessing": {
        "noise_reduction": True,
        "auto_gain_control": True,
        "voice_activity_detection": True,
        "silence_removal": True,
        "normalization": "rms"
    },
    
    "feature_extraction": {
        "method": "mel_spectrogram",
        "n_mels": 80,
        "n_fft": 400,
        "hop_length": 160,
        "window": "hann",
        "center": True,
        "normalize": True
    }
}

# ==================== ç¼“å­˜ä¼˜åŒ–é…ç½® ====================

CACHE_CONFIG = {
    "model_cache": {
        "enabled": True,
        "max_size_gb": 8,
        "ttl_hours": 24,
        "preload_models": True,
        "cache_location": "./models/cache"
    },
    
    "inference_cache": {
        "enabled": True,
        "max_entries": 1000,
        "hash_method": "md5",
        "cache_similar_inputs": True,
        "similarity_threshold": 0.95
    },
    
    "audio_cache": {
        "enabled": True,
        "max_size_mb": 500,
        "compress": True,
        "cache_processed": True
    }
}

# ==================== å¹¶å‘å¤„ç†é…ç½® ====================

CONCURRENCY_CONFIG = {
    "async_processing": {
        "enabled": True,
        "max_workers": min(8, psutil.cpu_count()),
        "queue_size": 100,
        "timeout_seconds": 30
    },
    
    "threading": {
        "audio_processing_threads": 2,
        "model_inference_threads": 1,  # é¿å…GILé—®é¢˜
        "io_threads": 4
    },
    
    "multiprocessing": {
        "enabled": False,  # GPUæ¨¡å‹ä¸é€‚ç”¨å¤šè¿›ç¨‹
        "processes": 1
    }
}

# ==================== ç›‘æ§ä¸è‡ªé€‚åº”é…ç½® ====================

MONITORING_CONFIG = {
    "performance_tracking": {
        "enabled": True,
        "metrics": [
            "inference_time",
            "gpu_utilization", 
            "memory_usage",
            "throughput",
            "queue_length"
        ],
        "log_interval": 10
    },
    
    "auto_optimization": {
        "enabled": True,
        "adapt_batch_size": True,
        "adapt_memory_usage": True,
        "adapt_worker_count": True,
        "optimization_interval": 60
    }
}

# ==================== ä¸“å®¶çº§ä¼˜åŒ–ç­–ç•¥ ====================

class UltraPerformanceOptimizer:
    """è¶…é«˜æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.gpu_info, self.cpu_info = detect_system_capabilities()
        self.config = get_ultra_fast_model_config()
        self.logger = logging.getLogger(__name__)
    
    def detect_system_capabilities(self):
        """æ£€æµ‹ç³»ç»Ÿèƒ½åŠ›ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        return detect_system_capabilities()
    
    def optimize_system(self):
        """ä¼˜åŒ–ç³»ç»Ÿï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        return self.apply_all_optimizations()
    
    def get_optimized_config(self):
        """è·å–ä¼˜åŒ–é…ç½®ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        return {
            "whisper_kwargs": {
                "torch_dtype": torch.float16,  # è¿™é‡Œè½¬æ¢ä¸ºå®é™…çš„torchç±»å‹
                "low_cpu_mem_usage": True,
                "use_safetensors": True
                # ç§»é™¤ trust_remote_code å’Œ device_mapï¼Œè¿™äº›å¯¹ Whisper pipeline ä¸é€‚ç”¨
            },
            "model_kwargs": self.config.get("model_optimization", {}),
            "gpu_kwargs": self.config.get("gpu_optimization", {}),
            "inference_kwargs": self.config.get("inference_optimization", {})
        }
    
    def optimize_torch_settings(self):
        """ä¼˜åŒ–PyTorchè®¾ç½®"""
        
        # å¯ç”¨ä¼˜åŒ–æ¨¡å¼
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # å®‰å…¨è®¾ç½®çº¿ç¨‹æ•°ï¼ˆé¿å…é‡å¤è®¾ç½®é”™è¯¯ï¼‰
        try:
            torch.set_num_threads(self.cpu_info['cores'])
            self.logger.info(f"PyTorch intra-opçº¿ç¨‹æ•°è®¾ç½®ä¸º: {self.cpu_info['cores']}")
        except RuntimeError as e:
            if "cannot be called after" in str(e) or "already initialized" in str(e):
                self.logger.warning("PyTorch intra-opçº¿ç¨‹æ•°å·²è¢«è®¾ç½®ï¼Œè·³è¿‡é‡å¤è®¾ç½®")
            else:
                self.logger.error(f"è®¾ç½®PyTorch intra-opçº¿ç¨‹æ•°æ—¶å‡ºé”™: {e}")
        
        try:
            torch.set_num_interop_threads(self.cpu_info['cores'])
            self.logger.info(f"PyTorch inter-opçº¿ç¨‹æ•°è®¾ç½®ä¸º: {self.cpu_info['cores']}")
        except RuntimeError as e:
            if "cannot set number of interop threads after parallel work has started" in str(e):
                self.logger.warning("PyTorch inter-opçº¿ç¨‹æ•°æ— æ³•è®¾ç½®ï¼ˆå¹¶è¡Œå·¥ä½œå·²å¼€å§‹ï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
            elif "cannot be called after" in str(e) or "already initialized" in str(e):
                self.logger.warning("PyTorch inter-opçº¿ç¨‹æ•°å·²è¢«è®¾ç½®ï¼Œè·³è¿‡é‡å¤è®¾ç½®")
            else:
                self.logger.error(f"è®¾ç½®PyTorch inter-opçº¿ç¨‹æ•°æ—¶å‡ºé”™: {e}")
        
        # å†…å­˜ç®¡ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.memory.set_per_process_memory_fraction(0.85)
        
        self.logger.info("PyTorchä¼˜åŒ–è®¾ç½®å·²åº”ç”¨")
    
    def setup_gpu_optimization(self):
        """è®¾ç½®GPUä¼˜åŒ–"""
        
        if not torch.cuda.is_available():
            return
        
        # è®¾ç½®GPUè®¾å¤‡
        if self.gpu_info['count'] > 0:
            torch.cuda.set_device(0)
        
        # å¯ç”¨æ··åˆç²¾åº¦
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
        os.environ['TORCH_USE_CUDA_DSA'] = '1'
        
        self.logger.info(f"GPUä¼˜åŒ–è®¾ç½®å®Œæˆï¼Œå¯ç”¨GPU: {self.gpu_info['count']}")
    
    def get_optimal_batch_size(self, model_size_gb: float = 3.0) -> int:
        """æ ¹æ®GPUå†…å­˜è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        
        if not self.gpu_info['available']:
            return 1
        
        available_memory = self.gpu_info['memory'][0] if self.gpu_info['memory'] else 8
        # é¢„ç•™40%å†…å­˜ç»™æ¨¡å‹ï¼Œå…¶ä½™ç”¨äºæ‰¹å¤„ç†
        batch_memory = (available_memory - model_size_gb) * 0.6
        
        # æ¯ä¸ªæ ·æœ¬å¤§æ¦‚éœ€è¦0.5GBå†…å­˜
        optimal_batch = max(1, int(batch_memory / 0.5))
        
        return min(optimal_batch, 16)  # æœ€å¤§16
    
    def apply_all_optimizations(self):
        """åº”ç”¨æ‰€æœ‰ä¼˜åŒ–"""
        
        self.optimize_torch_settings()
        self.setup_gpu_optimization()
        
        # åº”ç”¨ç¯å¢ƒå˜é‡ä¼˜åŒ–
        optimizations = {
            'OMP_NUM_THREADS': str(self.cpu_info['cores']),
            'MKL_NUM_THREADS': str(self.cpu_info['cores']),
            'NUMEXPR_NUM_THREADS': str(self.cpu_info['cores']),
            'TOKENIZERS_PARALLELISM': 'false',
            'CUDA_VISIBLE_DEVICES': '0,1' if self.gpu_info['count'] >= 2 else '0',
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512',
            'TRANSFORMERS_OFFLINE': '1',
            'HF_DATASETS_OFFLINE': '1'
        }
        
        for key, value in optimizations.items():
            os.environ[key] = value
        
        self.logger.info("ğŸš€ æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–å·²åº”ç”¨ï¼")

# ==================== æ™ºèƒ½é¢„åŠ è½½ç³»ç»Ÿ ====================

class SmartPreloader:
    """æ™ºèƒ½æ¨¡å‹é¢„åŠ è½½ç³»ç»Ÿ"""
    
    def __init__(self):
        self.preloaded_models = {}
        self.usage_stats = {}
    
    async def preload_critical_models(self):
        """é¢„åŠ è½½å…³é”®æ¨¡å‹"""
        
        models_to_preload = [
            "Qwen/Qwen2.5-Omni-3B",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ]
        
        for model_name in models_to_preload:
            try:
                # è¿™é‡Œå°†åœ¨å®é™…å®ç°ä¸­åŠ è½½æ¨¡å‹
                self.preloaded_models[model_name] = f"preloaded_{model_name}"
                logging.info(f"âœ… é¢„åŠ è½½æ¨¡å‹: {model_name}")
            except Exception as e:
                logging.error(f"âŒ é¢„åŠ è½½å¤±è´¥: {model_name}, é”™è¯¯: {e}")
    
    def start_preloading(self):
        """å¯åŠ¨é¢„åŠ è½½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # ç®€åŒ–çš„åŒæ­¥é¢„åŠ è½½
            logging.info("ğŸ“¡ å¯åŠ¨æ™ºèƒ½é¢„åŠ è½½ç³»ç»Ÿ...")
            return True
        except Exception as e:
            logging.error(f"é¢„åŠ è½½å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def preload_model(self, model, model_type: str):
        """é¢„åŠ è½½æŒ‡å®šæ¨¡å‹"""
        try:
            model_key = f"{model_type}_{id(model)}"
            self.preloaded_models[model_key] = model
            logging.info(f"âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆ: {model_type}")
            
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            if model_type not in self.usage_stats:
                self.usage_stats[model_type] = {"count": 0, "last_used": time.time()}
            self.usage_stats[model_type]["count"] += 1
            self.usage_stats[model_type]["last_used"] = time.time()
            
            return True
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {model_type}, é”™è¯¯: {e}")
            return False

# ==================== å®æ—¶æ€§èƒ½ç›‘æ§ ====================

class RealTimeMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'inference_times': [],
            'gpu_utilization': [],
            'memory_usage': [],
            'throughput': 0
        }
    
    def log_inference_time(self, time_ms: float):
        """è®°å½•æ¨ç†æ—¶é—´"""
        self.metrics['inference_times'].append(time_ms)
        if len(self.metrics['inference_times']) > 100:
            self.metrics['inference_times'].pop(0)
    
    def get_average_inference_time(self) -> float:
        """è·å–å¹³å‡æ¨ç†æ—¶é—´"""
        if not self.metrics['inference_times']:
            return 0
        return sum(self.metrics['inference_times']) / len(self.metrics['inference_times'])
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'avg_inference_time_ms': self.get_average_inference_time(),
            'total_inferences': len(self.metrics['inference_times']),
            'throughput_per_second': self.metrics['throughput'],
            'gpu_utilization': self.metrics['gpu_utilization'][-1] if self.metrics['gpu_utilization'] else 0
        }
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        try:
            logging.info("ğŸ“Š å¯åŠ¨å®æ—¶æ€§èƒ½ç›‘æ§...")
            return True
        except Exception as e:
            logging.error(f"ç›‘æ§å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        try:
            logging.info("ğŸ“Š åœæ­¢å®æ—¶æ€§èƒ½ç›‘æ§...")
            return True
        except Exception as e:
            logging.error(f"ç›‘æ§åœæ­¢å¤±è´¥: {e}")
            return False

# ==================== å¯¼å‡ºé…ç½® ====================

def get_production_config():
    """è·å–ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    
    optimizer = UltraPerformanceOptimizer()
    optimizer.apply_all_optimizations()
    
    return {
        "model_config": optimizer.config,
        "audio_config": AUDIO_PROCESSING_CONFIG,
        "cache_config": CACHE_CONFIG,
        "concurrency_config": CONCURRENCY_CONFIG,
        "monitoring_config": MONITORING_CONFIG,
        "optimizer": optimizer
    }

# ==================== æ€§èƒ½åŸºå‡†æµ‹è¯• ====================

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print("ğŸ å¯åŠ¨æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    gpu_info, cpu_info = detect_system_capabilities()
    
    print(f"ğŸ“Š ç³»ç»Ÿé…ç½®:")
    print(f"   CPU: {cpu_info['cores']}æ ¸å¿ƒ / {cpu_info['threads']}çº¿ç¨‹")
    print(f"   å†…å­˜: {cpu_info['memory_gb']}GB")
    print(f"   GPU: {gpu_info['count']}ä¸ªGPU")
    
    if gpu_info['memory']:
        for i, mem in enumerate(gpu_info['memory']):
            print(f"   GPU {i}: {mem}GBæ˜¾å­˜")
    
    config = get_ultra_fast_model_config()
    optimal_batch = UltraPerformanceOptimizer().get_optimal_batch_size()
    
    print(f"âœ¨ ä¼˜åŒ–é…ç½®:")
    print(f"   æ¨èæ‰¹å¤„ç†å¤§å°: {optimal_batch}")
    print(f"   é‡åŒ–æ¨¡å¼: {config['model_optimization']['quantization']['method']}")
    print(f"   å†…å­˜ä½¿ç”¨ç‡: {config['gpu_optimization']['memory_management']['max_memory_fraction']*100}%")
    
    return config

# ==================== å…¨å±€åˆå§‹åŒ–å‡½æ•° ====================

def initialize_ultra_performance():
    """
    å…¨å±€æ€§èƒ½ä¼˜åŒ–åˆå§‹åŒ–å‡½æ•°
    åº”åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œç¡®ä¿æ‰€æœ‰ä¼˜åŒ–é…ç½®æ­£ç¡®åº”ç”¨
    """
    
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–è¶…é«˜æ€§èƒ½ä¼˜åŒ–é…ç½®...")
    
    # è®¾ç½®HuggingFaceé•œåƒï¼ˆå…¨å±€è®¾ç½®ï¼‰
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    
    try:
        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
        optimizer = UltraPerformanceOptimizer()
        
        # åº”ç”¨PyTorchä¼˜åŒ–
        optimizer.optimize_torch_settings()
        
        # è®¾ç½®GPUä¼˜åŒ–
        optimizer.setup_gpu_optimization()
        
        # å¯åŠ¨é¢„åŠ è½½å™¨
        preloader = SmartPreloader()
        preloader.start_preloading()
        
        # å¯åŠ¨å®æ—¶ç›‘æ§
        monitor = RealTimeMonitor()
        monitor.start_monitoring()
        
        print("âœ… è¶…é«˜æ€§èƒ½ä¼˜åŒ–é…ç½®åˆå§‹åŒ–å®Œæˆï¼")
        
        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        display_system_info()
        
        return {
            'optimizer': optimizer,
            'preloader': preloader,
            'monitor': monitor,
            'status': 'initialized'
        }
        
    except Exception as e:
        logging.error(f"æ€§èƒ½ä¼˜åŒ–åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"âŒ æ€§èƒ½ä¼˜åŒ–åˆå§‹åŒ–å¤±è´¥: {e}")
        return {
            'status': 'failed',
            'error': str(e)
        }

# ==================== è‡ªåŠ¨åˆå§‹åŒ– ====================

# å®šä¹‰å…¨å±€å˜é‡å­˜å‚¨ä¼˜åŒ–ç»„ä»¶
_PERFORMANCE_COMPONENTS = None

def get_performance_components():
    """è·å–æ€§èƒ½ä¼˜åŒ–ç»„ä»¶ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _PERFORMANCE_COMPONENTS
    
    if _PERFORMANCE_COMPONENTS is None:
        _PERFORMANCE_COMPONENTS = initialize_ultra_performance()
    
    return _PERFORMANCE_COMPONENTS

# å¯¼å‡ºä¸»è¦ç±»å’Œå‡½æ•°
__all__ = [
    'UltraPerformanceOptimizer',
    'SmartPreloader', 
    'RealTimeMonitor',
    'initialize_ultra_performance',
    'get_performance_components',
    'detect_system_capabilities',
    'get_ultra_fast_model_config',
    'display_system_info'
]
